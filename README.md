## Micrograd
*_Micrograd.ipynb_*

Consists class Value written to define a Neuron. Also I use graphviz to visualize the gradient flow.

_mlp_bigram.ipynb:_

MultiLayer Perceptron on a bigram model trained with Shakespeare's data. 

Just so we could compare the text generated by this and our future transformer and appreciate how great the attention mechanism works.  


_shakesphere_transformer.ipynb:_ 

Has MultiHead Attention class built on top of our classes in Micrograd and bigram model.

Implements a Transformer on Shakespeare's text. To see some amazing coherent results.
