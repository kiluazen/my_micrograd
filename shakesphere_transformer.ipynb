{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is done from learnings of Andrej Karparthy's Tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-21 19:55:33--  https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
      "Resolving ocw.mit.edu (ocw.mit.edu)... 151.101.194.133, 151.101.130.133, 151.101.66.133, ...\n",
      "Connecting to ocw.mit.edu (ocw.mit.edu)|151.101.194.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5458199 (5.2M) [text/plain]\n",
      "Saving to: ‘t8.shakespeare.txt’\n",
      "\n",
      "t8.shakespeare.txt  100%[===================>]   5.21M   116KB/s    in 38s     \n",
      "\n",
      "2023-06-21 19:56:14 (142 KB/s) - ‘t8.shakespeare.txt’ saved [5458199/5458199]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t8.shakespeare.txt','r',encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of characters in the file: 5448282\n"
     ]
    }
   ],
   "source": [
    "total_chars = len(text)\n",
    "print(\"Total no of characters in the file:\", total_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "context_length = 24\n",
    "ch_in = {ch:i for i,ch in enumerate(chars)}\n",
    "in_ch = {i:ch for ch,i in ch_in.items()}\n",
    "\n",
    "encode = lambda s: [ch_in[ch] for ch in s]\n",
    "decode = lambda s: [in_ch[id.item()] for id in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(0.8*total_chars)\n",
    "n2 = int(0.9*total_chars)\n",
    "train_data  =   torch.tensor(encode(text[:n1]))\n",
    "val_data    =   torch.tensor(encode(text[n1:n2]))\n",
    "test_data   =   torch.tensor(encode(text[n2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_maker(data, batch_size):\n",
    "    idx = torch.randint(len(data) - batch_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+1+context_length] for i in idx])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from torch.nn.parameter import Parameter\n",
    "class Bigrammodel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,vocab_size)\n",
    "    # def embedding(self,x):\n",
    "    #     self.embed = nn.Embedding(vocab_size,vocab_size)\n",
    "    #     self.emb = self.embed(x)\n",
    "    #     return self.emb\n",
    "    def forward(self,x,y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        scores = self.embedding(self.x)\n",
    "        if self.y is None:\n",
    "            return scores\n",
    "        B,T,C = scores.shape\n",
    "        loss = F.cross_entropy(scores.view(B*T,C), self.y.view(B*T,))\n",
    "        return loss\n",
    "    def generate(self, max_tokens, idx):\n",
    "        input = idx\n",
    "        for i in range(max_tokens):\n",
    "            scores = self.forward(input) #B,T,C\n",
    "            # in a batch the C values of the last index in T holds the prob for the next term\n",
    "            probs = F.softmax(scores[:,-1,:], dim=-1)# this of shape B,C\n",
    "            self.probs = probs\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # of shape B,1\n",
    "            input = torch.concat((input,next_idx),dim=1)\n",
    "        self.out = input\n",
    "        return self.out\n",
    "    # def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
    "    #     return [self.embed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32         #embeddings\n",
    "n_head = 6         # for multihead\n",
    "head_size = 20      #the shape of key, query and value is n_emb,head_size\n",
    "d = 0.4            #dropout\n",
    "hidden_dim = 150    #affine layer dimensions\n",
    "\n",
    "n_layer = 2         # no of blocks\n",
    "batch_size = 100    # B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size)\n",
    "        self.query = nn.Linear(n_embd, head_size)\n",
    "        self.value = nn.Linear(n_embd, head_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        key =   self.key(x) #B,T,C ; C,H -> B,T,H\n",
    "        query = self.query(x) # B,T,H\n",
    "        value = self.value(x) # B,T,H\n",
    "        weights = key @ query.transpose(1,2) # B,T,H ; B,H,T -> B,T,T\n",
    "        B,T,T = weights.shape\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        weights[:,tril == 0] = float('-inf')\n",
    "        logits = F.softmax(weights, dim=-1)\n",
    "        out = logits @ value    \n",
    "        return out # B,T,H\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)]) # a list with different heads\n",
    "        self.linear = nn.Linear(n_head*head_size, n_embd)\n",
    "        self.drop = nn.Dropout(p = 0.3)\n",
    "    def forward(self, x):\n",
    "        head_forward = [head(x) for head in self.heads]\n",
    "        out = torch.concat(head_forward, dim=-1) # B,T,n_head*H\n",
    "        out = self.linear(out)\n",
    "        out = self.drop(out)\n",
    "        return out\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self , hidden_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Linear(n_embd, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_embd), nn.Dropout(d)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out = self.sequence(x)\n",
    "        return out\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_head, head_size, hidden_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHeadAttention(n_head, head_size)\n",
    "        self.feedforward = FeedForward(hidden_dim)\n",
    "        self.ly1 = nn.LayerNorm(n_embd)\n",
    "        self.ly2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self,x):\n",
    "        x = x+ self.multihead(self.ly1(x)) # norm on x then passed through multihead.\n",
    "        x = x+ self.feedforward(self.ly2(x)) # output of multihead is normed and passed through feedforward.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_layer) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(context_length, n_embd)\n",
    "        self.blocks = nn.Sequential( *[Block(n_head, head_size, hidden_dim) for _ in range(n_layer)]) \n",
    "        self.ly = nn.LayerNorm(n_embd)\n",
    "        self.fd = nn.Linear(n_embd,vocab_size)\n",
    "\n",
    "    def forward(self,x,y=None):\n",
    "        B,T = x.shape\n",
    "        tok_emd = self.embedding_table(x) # B,T,C\n",
    "        pos_emd = self.pos_embedding(torch.arange(T)) # T,C\n",
    "        x = tok_emd + pos_emd\n",
    "        x = self.blocks(x)\n",
    "        scores = self.fd(self.ly(x)) # B,T,vocab_size\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = scores.shape\n",
    "            logits = scores.view(B*T,-1)\n",
    "            targets = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return scores, loss\n",
    "\n",
    "    def generate(self, idx, max_tokens):\n",
    "        # idx B,T ; the output is gonna be B, T+max_tokens\n",
    "        for _ in range(max_tokens):\n",
    "            # since in the loop the length of idx gonna change, but we on want T length of idx\n",
    "            idx_last = idx[:, -context_length:] # makes idx back to B,T\n",
    "            scores,loss = self.forward(idx_last)    # B,T,C\n",
    "            logits = scores[:, -1, :] # the prediction is in the last value in the scores tensor # shape i B,C\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # B,1\n",
    "            idx = torch.concat((idx,idx_next), dim=-1) # B,T+1....\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model , n_itrs, print_every = 100):\n",
    "    \n",
    "    optim = torch.optim.AdamW(model.parameters(),lr = 1e-3)\n",
    "    for i in range(n_itrs):\n",
    "        x, y = data_maker(train_data,batch_size)\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        scores , loss = model(x,y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i%print_every == 1:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Transformer(n_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5606625080108643\n",
      "2.695490598678589\n",
      "2.5984342098236084\n",
      "2.8738484382629395\n",
      "3.1028683185577393\n",
      "3.1389927864074707\n",
      "3.2499146461486816\n",
      "3.342510223388672\n",
      "3.5604560375213623\n",
      "3.4358527660369873\n",
      "4.129700183868408\n",
      "3.612978935241699\n",
      "3.0874104499816895\n",
      "3.8182787895202637\n",
      "3.3055593967437744\n",
      "4.378464698791504\n",
      "5.024666786193848\n",
      "5.140851974487305\n",
      "6.387229919433594\n",
      "5.879176139831543\n",
      "5.57369327545166\n",
      "5.44919490814209\n",
      "5.55044412612915\n",
      "5.521071910858154\n",
      "4.852461338043213\n",
      "4.9375505447387695\n",
      "5.650205612182617\n",
      "5.764904022216797\n",
      "6.035919666290283\n",
      "6.549779891967773\n",
      "7.837399005889893\n",
      "7.895719528198242\n",
      "6.469213485717773\n",
      "6.286374568939209\n",
      "9.014177322387695\n",
      "8.452606201171875\n",
      "8.766841888427734\n",
      "8.80438232421875\n",
      "7.6910905838012695\n",
      "8.21726131439209\n"
     ]
    }
   ],
   "source": [
    "training(t, 10000, lr = 5e-3, print_every=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n",
      "    m,mra eearb  esarae  uUU nb  i-a aa w-U rR eU r rb  ho r b b e U r i uU G   rrUarGsb fmGtui  m se aGr eb s  eGiib b fesRb  uUe tue  .e bR   b   sari n W taheU U  i e U mr;r U - eubRb,r  amea b  Ghaa am ir   e m eUbbbee-UcG hU-aUUeUerere  UUa-hb- U- i ua,bUr   mee w, beaal UUUaeam   R abraew b rU bs uU;  - Yr,rru ,b eeU   rUU n aarU  -     su dre Ub   wm bGe  U s b be a  ,uPm  aUuuberuUsitasbbouta ie  b'uo   b  Ui  G rua U be   Ue eYsiUGrbbaeraU  awai l waGs atU btl r wUeG  ie r  i rabc e  ebUe  b, sw Ubr Ur -U  bbh rba ab   srUe  U   beu eei  ae nbG R  ir U  G,oUaGr aa errw u bU-   r tea rr bu  agehaea   rialrU aaUW sae a  b G U rrUr   rba ub a, ,,i tRr    U   UirUCU    b  Wa abUeireUua  iU -e   r a  rl t eu  mbsUereee-e w sUa tserue r aGbirmrrr raR e     a U n e ie  eeaU bUeba-mebebeG - U\n"
     ]
    }
   ],
   "source": [
    "x ,y = data_maker(test_data, batch_size=1)\n",
    "idx = t.generate(x, max_tokens=800)\n",
    "text = ''.join(decode(idx.squeeze()))\n",
    "print(text)\n",
    "# for id in list(idx):\n",
    "#     for i in id:\n",
    "#         print(decode(i.item()))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   query = self.query(query)\n",
    "#         key = self.key(key)\n",
    "#         value = self.value\n",
    " \n",
    "# Ki = key.reshape(N,T,H,E//H).transpose(1,2).transpose(2,3)  # N,H,E//H,T   \n",
    "#         query_weights = torch.matmul(Qi, Ki)\n",
    "#         assert query_weights.shape == (N,H,T,T)\n",
    "#         Vi = value.reshape(N,T,H,E//H).transpose(1,2) # N,H,T,E//H\n",
    "#         Yi = torch.matmul(query_weights, Vi)           #N,H,T,E//H\n",
    "#         Y = Yi.transpose(1,2).reshape(N,T,E)\n",
    "#         output = self.proj(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
